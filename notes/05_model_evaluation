------------------------------------------------------
CHAPTER 05 - MODEL EVALUATION
------------------------------------------------------

- Cross-validation, grid search, and evaluation metrics are the cornerstones of evaluating and
    improving machine learning algorithms.



- There are 2 important points from this chapter which are often overlooked by new practitioners:

    1. Cross-validation or the use of a test set allow us to evaluate a machine learning 
         algorithm as it will perform in the future.  However, if we use the test set or cross
         validation to select a model or select the model parameters, we "use up" the test data,
         and using the same data to evaluate how well our model will do in the future will lead
         to overly optimistic estimates.

       We therefore need to resort to a split into training data for model building, validation
         data for model and parameter selection, and test data for model evaluation.  Instead
         of a simple split, we can replace each of these splits with cross-validation.  The most
         commonly used form is a training/test split for evaluation, and using cross-validation
         on the training set for model and parameter selection.


    2. It is rarely the case that the end result of a machine learning task is building a model
        with high accuracy.  Make sure the metric you choose to evaluate and select a model for
        is a good stand-in for what the model will actually be used for.  In reality, 
        classification problems rarely have balanced classes, and often false positives and
        false negatives have very different consequences.  Make sure you understand what these
        consequences are, and pick an evaluation metric accordingly.