------------------------------------------------------
CHAPTER 02 - SUPERVISED LEARNING
------------------------------------------------------

- Classification and Regression

    There are 2 major types of supervised machine learning problems, classification and
      regression.  

    In classification, the goal is predict a class label, which is a choice from a 
      predefined list of possibilities.  Classification is separated into 'binary 
      classification', which is a special case of distinguishing between 2 classes, 
      and 'multiclass classification', which is classification between more than
      2 classes.

    For regression tasks, the goal is to predict a continuous number (aka a floating point
      or real number).  Predicting someone's income from their education, age, and where
      they live is an example.  Predicting the yield of a corn farm based on previous 
      yields, weather, and number of employees is another example.

    An easy way to distinguish between classification and regression tasks is to ask
      whether there is some kind of continuity in the output.  If there is continuity, the
      problem is a regression problem.  



- Generalization, Overfitting, and Underfitting

    If a model is able to make accurate predictions on new, unseen data, we say it is able
      to 'generalize' from the training set to the test set.  We want to build a model that 
      is able to generalize as accurately as possible.

    Usually, we build a model in such a way that it can make accurate predictions on the
      training set.  If the training on test sets have enough in common, we then expect the
      model to also be accurate on the test set.  However, there are some cases in which this
      can go wrong.  For instance, we can always produce a very complex model to be as accurate
      as we like on the training set.  This is known as 'overfitting', which is when you
      fit a model to closely to the particularities of the traing set, and the model is
      unable to generalize to new data.

    Choosing too simple a model is called 'underfitting', and in this case we may not be able
      to capture all the aspects of the data.

    The more complex we allow our model to be, the better we will be able to predict on the
      training data.  However, if our model becomes too complex, the model will not 
      generalize well to new data.  There is a sweet spot in between that will yield the
      best generalization performance.  This is the model we want to find.

    Model complexity is intimately tied to the variation of inputs contained in our
      training sets.  The larger the variety of data points your data set contains,
      the more complex a model you can use without overfitting.  Never underestimate
      the power of more data!