{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so many ways to create new features, you might get tempted to increase the dimensionality of the data way beyond the number of original features. However, adding more features makes all models more complex, and so increases the chance of overfitting. When adding new features, or with high-dimensional datasets in general, it can be a good idea to reduce the number of features to only the most useful ones, and discard the rest. This can lead to simpler models that generalize better. \n",
    "\n",
    "But how can you know how good each feature is? There are three basic strategies: *univariate statistics*, *model-based selection*, and *iterative selection*. We will discuss all three of them in detail. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and test sets, and fit the feature selection only on the training part of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In univariate statistics, we compute whether there is a statistically significant relationship between each feature and the target. Then the features that are related with the highest confidence are selected. In the case of classification, this is also known as *analysis of variance (ANOVA)*. \n",
    "\n",
    "A key property of these tests is that they are *univariate*, meaning that they only consider each feature individually. Consequently, a feature will be discarded if it is only informative when combined with another feature. Univariate tests are often very fast to compute, and don’t require building a model. On the other hand, they are completely independent of the model that you might want to apply after the feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use univariate feature selection in scikit-learn, you need to choose a test, usually either *f_classif* (the default) for classification or f_regression for regression, and a method to discard features based on the *p-values* determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target). \n",
    "\n",
    "The methods differ in how they compute this threshold, with the simplest ones being *SelectKBest*, which selects a fixed number k of features, and *SelectPercentile*, which selects a fixed percentage of features. \n",
    "\n",
    "Let’s apply the feature selection for classification on the *cancer* dataset. To make the task a bit harder, we’ll add some noninformative noise features to the data. We expect the feature selection to be able to identify the features that are noninformative and remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from IPython.display import display\n",
    "import mglearn\n",
    "\n",
    "# Don't display deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (284, 80)\n",
      "X_train_selected.shape: (284, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Get deterministic random numbers\n",
    "rng = np.random.RandomState(42)\n",
    "noise = rng.normal(size=(len(cancer.data), 50))\n",
    "\n",
    "# Add noise features to the data\n",
    "# The first 30 features are from the dataset, the next 50 are noise\n",
    "X_w_noise = np.hstack([cancer.data, noise])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
    "\n",
    "# Use f_classif (the default) and SelectPercentile to select 50% of features\n",
    "select = SelectPercentile(percentile=50)\n",
    "select.fit(X_train, y_train)\n",
    "\n",
    "# Transform training set\n",
    "X_train_selected = select.transform(X_train)\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of features was reduced from 80 to 40 (50 percent of the original number of features). We can find out which features have been selected using the *get_support* method, which returns a Boolean mask of the selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True False False False  True False  True\n",
      " False False  True False False False False  True False False  True False\n",
      " False  True False  True False False False False False False  True False\n",
      "  True False False False False  True False  True False False False False\n",
      "  True  True False  True False False False False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = select.get_support()\n",
    "print(mask)\n",
    "\n",
    "# Visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the visualization of the mask, most of the selected features are the original features, and most of the noise features were removed. However, the recovery of the original features is not perfect. Let’s compare the performance of logistic regression on all features against the performance using only the selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with all features: 0.930\n",
      "Score with only selected features: 0.940\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Transform test data\n",
    "X_test_selected = select.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "lr.fit(X_train_selected, y_train)\n",
    "print(\"Score with only selected features: {:.3f}\".format(\n",
    "    lr.score(X_test_selected, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, removing the noise features improved performance, even though some of the original features were lost. This was a very simple synthetic example, and outcomes on real data are usually mixed. Univariate feature selection can still be very helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
