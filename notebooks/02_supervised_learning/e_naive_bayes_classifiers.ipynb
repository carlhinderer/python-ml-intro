{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Naive Bayes* classifiers are a family of classifiers that are quite similar to linear models.  However, they tend to be even faster in training.  The price paid for this efficiency is that Naive Bayes models often provide generalization performance that is slightly worse than *LogisticRegression* and *LinearSVC*.\n",
    "\n",
    "The reason that Naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature.  \n",
    "\n",
    "There are 3 kinds of Naive Bayes classifiers implemented in scikit-learn:\n",
    "- GaussianNB (can be applied to any continuous data)\n",
    "- BernoulliNB (assumes binary data)\n",
    "- MultinomialNB (assumes integer count data, like how many times a word appears in a sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BernoulliNB Classifier counts how often every feature of each class is not zero.\n",
    "\n",
    "Here, we have 4 data points, with 4 binary features each.  There are 2 classes, 0 and 1.  \n",
    "\n",
    "For class 0 (the first and third data points):\n",
    "- the first feature is zero 2 times and nonzero 0 times\n",
    "- the second feature is zero 1 time and nonzero 1 time\n",
    "- the third feature is zero 2 times and nonzero 0 times\n",
    "- the fourth feature is zero 0 times and nonzero 2 times\n",
    "\n",
    "For class 1 (the second and fourth data points):\n",
    "- the first feature is zero 0 times and nonzero 2 times\n",
    "- the second feature is zero 2 times and nonzero 0 times\n",
    "- the third feature is zero 0 times and nonzero 2 times\n",
    "- the fourth feature is zero 1 time and nonzero 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from IPython.display import display\n",
    "import mglearn\n",
    "\n",
    "# Don't display deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the nonzero entries per class in essence looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      " {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # Iterate over each class\n",
    "    # Count (sum) entries of 1 per feature\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "    \n",
    "print(\"Feature counts:\\n\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two Naive Bayes models, *MultinomialNB* and *GaussianNB*, are slightly different in what kind of statistics they compute.  *MultinomialNB* takes into account the average value of each feature for each class, while *GaussianNB* stores the average value as well as the standard deviation of each feature for each class.\n",
    "\n",
    "To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted.  Interestingly, for both *MultinomialNB* and *BernoulliNB*, this leads to a prediction formula that is the same as the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths, Weaknesses, and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MultinomialNB* and *BernoulliNB* have a single parameter, *alpha*, which controls model complexity.  The way *alpha* works is that the algorithm adds to the data *alpha* many virtual data points that have positive values for all the features.  The results in a \"smoothing\" of the statistics.\n",
    "\n",
    "A large *alpha* means more smoothing, resulting in less complex models.  Setting *alpha* isn't critical for good performance, but tuning it usually improves accuracy somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GaussianNB* is mostly used on very high-dimensional data, while the other 2 variants of Naive Bayes are widely used for sparse data such as text.  *MultinomialNB* usually performs better than *BernoulliNB*, particularly on datasets with a relatively large number of nonzero features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes models share many of the strengths and weaknesses of the linear models.  They are very fast to train and predict, and the training procedure is easy to understand.  The models work very well with high-dimensional sparse data and are relatively robust to the parameters.  Naive Bayes models are great baseline models and are often used on very large datasets, where training even a linear model might take too long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
