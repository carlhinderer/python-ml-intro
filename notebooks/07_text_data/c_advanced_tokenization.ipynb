{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tokenization, Stemming, and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, the feature extraction in the *CountVectorizer* and *TfidfVectorizer* is relatively simple, and much more elaborate methods are possible. One particular step that is often improved in more sophisticated text-processing applications is the first step in the bag-of-words model: tokenization. This step defines what constitutes a word for the purpose of feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the vocabulary often contains singular and plural versions of some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and \"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only increase overfitting, and not allow the model to fully exploit the training data. \n",
    "\n",
    "Similarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replacement\", \"replaces\", and \"replacing\", which are different verb forms and a noun relating to the verb “to replace.” Similarly to having singular and plural forms of a noun, treating different verb forms and related words as distinct tokens is disadvantageous for building a model that generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be overcome by representing each word using its *word stem*, which involves identifying (or *conflating*) all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as *stemming*. \n",
    "\n",
    "If instead a dictionary of known word forms is used (an explicit and human-verified system), and the role of the word in the sentence is taken into account, the process is referred to as *lemmatization* and the standardized form of the word is referred to as the *lemma*. \n",
    "\n",
    "Both processing methods, lemmatization and stemming, are forms of normalization that try to extract some normal form of a word. Another interesting case of normalization is spelling correction, which can be helpful in practice but is outside of the scope of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of normalization, let’s compare a method for stemming—the Porter stemmer, a widely used collection of heuristics (here imported from the *nltk* package)—to lemmatization as implemented in the *spacy* package:\n",
    "\n",
    "To run this code, you need to install both *nltk* and *spacy*, and also install the English language support for *spacy*:\n",
    "\n",
    "`pip install nltk spacy\n",
    "python -m spacy download en`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the following versions of the two libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from IPython.display import display\n",
    "import mglearn\n",
    "\n",
    "# Don't display deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy version: 2.1.4\n",
      "nltk version: 3.4.3\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(\"SpaCy version: {}\".format(spacy.__version__))\n",
    "\n",
    "import nltk\n",
    "print(\"nltk version: {}\".format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy's English-language models\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "# Instantiate nltk's Porter stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# Define function to compare lemmatization in spacy with stemming in nltk\n",
    "def compare_normalization(doc):\n",
    "    # Tokenize document in spacy\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # Print lemmas found by spacy\n",
    "    print(\"Lemmatization:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    # Print tokens found by Porter stemmer\n",
    "    print(\"Stemming:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare lemmatization and the Porter stemmer on a sentence designed to show some of the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['-PRON-', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', '-PRON-', 'be', 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', 'am', 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \"\n",
    "                       \"I'm scared of meeting the clients tomorrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is always restricted to trimming the word to a stem, so \"was\" becomes \"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly, lemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\". \n",
    "\n",
    "Another major difference is that stemming reduces both occurrences of \"meeting\" to \"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a noun and left as is, while the second occurrence is recognized as a verb and reduced to \"meet\". \n",
    "\n",
    "In general, lemmatization is a much more involved process than stemming, but it usually produces better results than stemming when used for normalizing tokens for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While scikit-learn implements neither form of normalization, *CountVectorizer* allows specifying your own tokenizer to convert each document into a list of tokens using the *tokenizer* parameter. We can use the lemmatization from spacy to create a callable that will take a string and produce a list of lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "\n",
    "# Load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technicality: we want to use the regexp-based tokenizer\n",
    "# that is used by CountVectorizer and only use the lemmatization\n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
    "# with the regexp-based tokenization.\n",
    "\n",
    "import re\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# Load spacy language model\n",
    "en_nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "\n",
    "# Replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n",
    "    regexp.findall(string))\n",
    "\n",
    "# Create a custom tokenizer using the spacy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# Define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s transform the data and inspect the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lemma.shape: (25000, 21571)\n",
      "X_train.shape: (25000, 27271)\n"
     ]
    }
   ],
   "source": [
    "# Transform text_train using CountVectorizer with lemmatization\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "\n",
    "# Standard CountVectorizer for reference\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output, lemmatization reduced the number of features from 27,271 (with the standard CountVectorizer processing) to 21,596. Lemmatization can be seen as a kind of regularization, as it conflates certain features. Therefore, we expect lemmatization to improve performance most when the dataset is small. \n",
    "\n",
    "To illustrate how lemmatization can help, we will use *StratifiedShuffleSplit* for cross-validation, using only 1% of the data as training data and the rest as test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (standard CountVectorizer): 0.721\n",
      "Best cross-validation score (lemmatization): 0.735\n"
     ]
    }
   ],
   "source": [
    "# Build a grid search using only 1% of the data as the training set\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99,\n",
    "                            train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "\n",
    "# Perform grid search with standard CountVectorizer\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "# Perform grid search with lemmatization\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(lemmatization): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, lemmatization provided a modest improvement in performance. As with many of the different feature extraction techniques, the result varies depending on the dataset. Lemmatization and stemming can sometimes help in building better (or at least more compact) models, so we suggest you give these techniques a try when trying to squeeze out the last bit of performance on a particular task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
